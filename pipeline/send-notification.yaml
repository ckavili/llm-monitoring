---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: send-notification
spec:
  params:
    - name: MODEL_NAME
      description: Name of the model
      type: string
    - name: TYPE
      description: The type of the model (openai, embedding, etc)
      type: string
  steps:
    - name: send-notification
      image: registry.redhat.io/ubi9/python-311@sha256:fc669a67a0ef9016c3376b2851050580b3519affd5ec645d629fd52d2a8b8e4a
      env:
        - name: slack-webhook
          valueFrom:
            secretKeyRef:
              name: slack-webhook
              key: "endpoint"
      script: |
        #!/bin/sh
        echo $(params.MODEL_NAME)
        echo $(params.TYPE)
        
        python3 -m pip install openai requests kubernetes
        cat << 'EOF' | python3
        import os
        import requests
        from openai import OpenAI, APIError, RateLimitError, APIConnectionError
        from kubernetes import client, config

        os.environ['SSL_CERT_FILE'] = '/tekton-custom-certs/ca-bundle.crt'
        slack_webhook_url = os.environ.get('slack-webhook')

        model_name = "$(params.MODEL_NAME)"

        # Load Kubernetes config (in-cluster or local)
        try:
            config.load_incluster_config()
        except config.ConfigException:
            config.load_kube_config()

        def get_deployment_logs(deployment_name: str, namespace: str, tail_lines: int = 5):
            config.load_incluster_config()
            
            apps_v1 = client.AppsV1Api()
            core_v1 = client.CoreV1Api()
            
            try:
                # Get the deployment to find its selector
                deployment = apps_v1.read_namespaced_deployment(name=deployment_name, namespace=namespace)
                
                # Get the selector from the deployment
                selector = deployment.spec.selector.match_labels
                
                # Convert the selector to a string format for the label selector
                label_selector = ",".join([f"{k}={v}" for k, v in selector.items()])
                
                # List pods with the matching labels
                pods = core_v1.list_namespaced_pod(namespace=namespace, label_selector=label_selector)
                
                if not pods.items:
                    return f"No pods found for deployment {deployment_name}"
                
                # Get logs from the first pod
                pod_name = pods.items[0].metadata.name
                logs = core_v1.read_namespaced_pod_log(name=pod_name, namespace=namespace, tail_lines=tail_lines)
                
                return logs.strip() if logs else "No logs available."
            
            except client.exceptions.ApiException as e:
                return f"Failed to fetch logs: {e}"

        deployment_name = f"{model_name}-predictor" 
        namespace = "llm-hosting"
        logs = get_deployment_logs(deployment_name, namespace)
        print(logs)

        # Define models and their respective API details
        models = {
            "mistral-7b-instruct": {
                "api_key": "xxx",
                "base_url": "https://mistral-7b-instruct-xxx.com:443/v1"
            },
            "granite-3-8b-instruct": {
                "api_key": "yyy",
                "base_url": "https://granite-3-8b-instruct-yyy.com:443/v1"
            }
        }

        responses = {}

        for model, details in models.items():
            try:
                client = OpenAI(api_key=details["api_key"], base_url=details["base_url"])
                
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                            "role": "user",
                            "content": f"Analyze the following error log and explain what went wrong. Identify the root cause and suggest possible solutions or troubleshooting steps. Keep it short and snappy.\n\n```\n{logs}\n```"
                        }
                    ],
                    max_tokens=500
                )

                responses[model] = response.choices[0].message.content

            except Exception as e:
                responses[model] = f"Error fetching response: {str(e)}"

        # Add disclaimer
        disclaimer = "âš ï¸ *Note: This information is generated by AI and may not be fully accurate. Please validate as needed.*"

        # Format responses for Slack
        formatted_responses = "\n\n".join(
            f"*Response from {model}:*\n{response}" for model, response in responses.items()
        )

        # Construct Slack message
        slack_message = f"""
        ðŸš¨ *{model_name}* is not responding!

        {disclaimer}

        ðŸ”® *Possible Causes:*

        {formatted_responses}

        ðŸ“š *Logs:*

        ```
        {logs}
        ```
        """

        slack_payload = {"text": slack_message}

        # Send to Slack webhook
        requests.post(slack_webhook_url, json=slack_payload)
